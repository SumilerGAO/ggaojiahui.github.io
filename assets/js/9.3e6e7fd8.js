(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{210:function(e,t,a){"use strict";var n=a(74);a.n(n).a},227:function(e,t,a){"use strict";a.r(t);a(210);var n=a(0),r=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"work"}},[e._v("Work")]),e._v(" "),a("p",[e._v("Here are some works of mine ðŸ“š")]),e._v(" "),a("h2",{attrs:{id:"projects"}},[e._v("Projects")]),e._v(" "),a("ProjectCard",{attrs:{image:"/projects/1.png"}},[a("p",[a("strong",[e._v("Jiahui Gao")]),e._v(", Zhou Yi,Gao, Philip LH Yu, Shafiq Joty, and Jiuxiang Gu")]),e._v(" "),a("p",[a("strong",[e._v("Unsupervised Cross-lingual Image Captioning")])]),e._v(" "),a("p",[e._v("Research in image captioning has mostly focused on English because of the availability of image-caption paired datasets in this language. However, building vision-language systems only for English deprives a large part of the world population of AI technologies' benefit. On the other hand, creating image-caption paired datasets for every target language is expensive. In this work, we present a novel unsupervised cross-lingual method to generate image captions in a target language without using any image-caption corpus in the source or target languages...")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://arxiv.org/pdf/2010.01288.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("PDF"),a("OutboundLink")],1),e._v("] ["),a("a",{attrs:{href:"https://arxiv.org/abs/2010.01288",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),a("OutboundLink")],1),e._v("]")])])],1)}),[],!1,null,null,null);t.default=r.exports},74:function(e,t,a){}}]);